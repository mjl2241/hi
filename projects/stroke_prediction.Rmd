---
title: "Stroke Prediction Using Demographic and Clinical Variables"
output: word_document
editor_options: 
  chunk_output_type: console
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  results = "asis"
)
```

#### Introduction

**Describe your data set. Provide proper motivation for your work.**
Although stroke is preventable and treatable, it is a leading cause of death and serious disability for adults in the United States (<https://www.cdc.gov/stroke/index.htm>). Globally, stroke causes over 6 million deaths per year. Also, 70% of strokes and 87% of both stroke-related deaths and disability occur in low and middle-income countries (<https://www.who.int/bulletin/volumes/94/9/16-181636.pdf>).Hence, predicting stroke cases based on certain health indicators might be beneficial in potentially preventing strokes in the world’s marginalized communities. Therefore, we will use Lasso, MARS, Random Forest, Boosting models to identify a stroke prediction model with the highest accuracy, and see which variables are important in making this prediction.

**What questions are you trying to answer?** 
Through this dataset, we want to know whether the demographic and clinical characteristics in the dataset can be used to predict stroke event accurately. If so, the results may provide inspiration for early detection and intervention for stroke. Thus, the main research question is: Can the demographic and clinical variables in the dataset be used to build a model for stroke prediction? How accurate is the model?

**How did you prepare and clean the data?**
We used data that were available
from the internet (<https://www.kaggle.com/fedesoriano/stroke-prediction-dataset>). The original dataset consists of 5110 observations and 12 variables including: id, gender (“Male”, “Female” or “Other”), age, hypertension (0 if the patient doesn’t have hypertension, 1 if the patient has hypertension), heart_disease(0 if the patient doesn’t have any heart diseases, 1 if the patient has a heart disease), marital status (“No” or “Yes”), work type (“children”, “Govt_jov”,“Never_worked”, “Private” or “Self-employed”), Residence type (“Rural” or “Urban”), avg_glucose_level, bmi, smoking_status (“formerly smoked”, “never smoked”, “smokes” or “Unknown”), stoke (1 if the patient had a stroke or 0 if not). We first prepared the data by dropping the “id” variable because it is irrelevant to our study. Out of 5110 observations, 1544 (30.2%) rows had missing values in `smoking_status` and 201 (3.9%) missing values in `bmi`. smoking status variable was excluded and other rows with missing data were dropping, resulting in 4908 observations and 10 total variables (ID and smoking status were dropped). We then changed each variable to factor or numeric, depending on variable type (continuous, categorical). We see that there is an imbalance of outcomes as the majority of participants did not have stroke, 3246 of no stroke and 180 of stroke cases, so we will oversample from the stroke cases to ensure good representation of outcome.

### Exploratory analysis/visualization

```{r package}
library(tidyverse)
library(caret)
library(summarytools)
library(corrplot)
library(knitr)
library(patchwork)
library(mgcv)
library(pROC)
library(vip)
library(ranger)

opts_chunk$set(
	fig.width = 12, 
  fig.asp = .8,
  out.width = "100%"
)

theme_set(theme_minimal() + theme(legend.position = "none"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r prep}
# Load and clean the data
stroke_df = 
  read_csv("./healthcare-dataset-stroke-data.csv") %>% 
  janitor::clean_names() %>% 
  select(-id) %>% 
  mutate(
    across(
      c("gender", "hypertension", "heart_disease", "ever_married", "work_type", "residence_type", "smoking_status", "stroke"),
      ~ as.factor(.x)
    )
  ) %>% 
  mutate(
    smoking_status = na_if(smoking_status, "Unknown"),
    bmi = na_if(bmi, "N/A"),
    bmi = as.numeric(bmi)
  )

# Summary of `stroke_df`
st_options(
  plain.ascii = FALSE,       
  style = "rmarkdown", 
  dfSummary.silent = TRUE,        
  footnote = NA,          
  subtitle.emphasis = FALSE
)      

dfSummary(stroke_df)
# Only 1 observation in the "Other" category of `gender`.
# 1544 (30.2%) missing values in `smoking_status` and 201 (3.9%) missing values in `bmi`.

# Further clean the data
stroke_clean = 
  stroke_df %>% 
  select(-smoking_status) %>% 
  filter(gender != "Other") %>% 
  mutate(gender = droplevels(gender)) %>% 
  mutate(
    across(c(hypertension, heart_disease, stroke), ~ as.factor(recode(.x, "1" = "Yes", "0" = "No"))),
    stroke = relevel(stroke, ref = "No")
  )

#delete omitted rows
stroke_clean <-na.omit(stroke_clean)

# Summary of `stroke_clean`
dfSummary(stroke_clean)
```

```{r}
# Correlation matrix among predictors
stroke_eda = 
  stroke_clean %>% 
  mutate(work_type = relevel(work_type, ref = "Govt_job"))
whole_x = model.matrix(stroke ~ ., stroke_eda)[ ,-1] 
corrplot(cor(whole_x), method = "circle", type = "full")
# Having ever been married seems to be associated with older age. No other apparent correlation between predictors.

# Density plots of continuous predictors across levels of “stroke” 
featurePlot(
  x = stroke_clean[ , c(2, 8, 9)], 
  y = stroke_clean$stroke,
  scales = list(
    x = list(relation = "free"), 
    y = list(relation = "free")
  ),
  plot = "density", pch = "|", 
  auto.key = list(columns = 2)
)

# Bar charts between `stroke` and categorical predictors
gender_bar = 
  stroke_clean %>% 
  count(gender, stroke) %>% 
  group_by(gender) %>% 
  mutate(stroke_prop = n / sum(n)) %>% 
  filter(stroke == "Yes") %>% 
  ggplot(aes(x = gender, y = stroke_prop, fill = gender)) +
    geom_col() +
    geom_text(aes(label = n), vjust = -0.5) +
    labs(
      y = "proportion of stroke event"
    )

hp_bar = 
  stroke_clean %>% 
  count(hypertension, stroke) %>% 
  group_by(hypertension) %>% 
  mutate(stroke_prop = n / sum(n)) %>% 
  filter(stroke == "Yes") %>% 
  ggplot(aes(x = hypertension, y = stroke_prop, fill = hypertension)) +
    geom_col() +
    geom_text(aes(label = n), vjust = -0.5) +
    labs(
      y = "proportion of stroke event"
    )

hd_bar = 
  stroke_clean %>% 
  count(heart_disease, stroke) %>% 
  group_by(heart_disease) %>% 
  mutate(stroke_prop = n / sum(n)) %>% 
  filter(stroke == "Yes") %>% 
  ggplot(aes(x = heart_disease, y = stroke_prop, fill = heart_disease)) +
    geom_col() +
    geom_text(aes(label = n), vjust = -0.5) +
    labs(
      y = "proportion of stroke event"
    )

married_bar = 
  stroke_clean %>% 
  count(ever_married, stroke) %>% 
  group_by(ever_married) %>% 
  mutate(stroke_prop = n / sum(n)) %>% 
  filter(stroke == "Yes") %>% 
  ggplot(aes(x = ever_married, y = stroke_prop, fill = ever_married)) +
    geom_col() +
    geom_text(aes(label = n), vjust = -0.5) +
    labs(
      y = "proportion of stroke event"
    )

work_bar = 
  stroke_clean %>% 
  count(work_type, stroke) %>% 
  group_by(work_type) %>% 
  mutate(stroke_prop = n / sum(n)) %>% 
  filter(stroke == "Yes") %>% 
  ggplot(aes(x = work_type, y = stroke_prop, fill = work_type)) +
    geom_col() +
    geom_text(aes(label = n), vjust = -0.5) +
    labs(
      y = "proportion of stroke event"
    )

residence_bar = 
  stroke_clean %>% 
  count(residence_type, stroke) %>% 
  group_by(residence_type) %>% 
  mutate(stroke_prop = n / sum(n)) %>% 
  filter(stroke == "Yes") %>% 
  ggplot(aes(x = residence_type, y = stroke_prop, fill = residence_type)) +
    geom_col() +
    geom_text(aes(label = n), vjust = -0.5) +
    labs(
      y = "proportion of stroke event"
    )

(gender_bar + hp_bar + hd_bar) / (married_bar + work_bar + residence_bar)
```

# Models

```{r}
# Exclude `work_type`
stroke_nowork = 
  stroke_clean %>% 
  select(-work_type)

# Data partition
set.seed(1)
train_indices = createDataPartition(
  y = stroke_nowork$stroke,
  p = 0.75,
  list = FALSE
)

# Imputation
set.seed(2)
stroke_x = stroke_nowork[train_indices, -9]
bagimp = preProcess(stroke_x, method = "bagImpute")
stroke_train = predict(bagimp, stroke_nowork[train_indices, ])
stroke_test = predict(bagimp, stroke_nowork[-train_indices, ])

# Train control
ctrl = trainControl(
  method = "cv", 
  number = 10, 
  sampling = "down", 
  summaryFunction = twoClassSummary, 
  classProbs = TRUE
)

# Model 1: LASSO
set.seed(111)
lasso_mod = train(
  stroke ~ .,
  data = stroke_train,
  method = "glmnet",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 1, 
                         lambda = exp(seq(-2, -7, length = 100)))
)

lasso_mod$bestTune

# Model 2: MARS
set.seed(123)
mars_mod = train(
  stroke ~ .,
  data = stroke_train,
  method = "earth",
  tuneGrid = expand.grid(degree = 1:3, 
                         nprune = 2:15),
  metric = "ROC",
  trControl = ctrl
)

mars_mod$bestTune

# Model 3: random forest
set.seed(222)
rf_mod = train(
  stroke ~ .,
  data = stroke_train,
  method = "ranger",
  tuneGrid = expand.grid(
    mtry = 1:8,
    splitrule = "gini",
    min.node.size = 1:6
  ),
  metric = "ROC",
  trControl = ctrl
)

rf_mod$bestTune

# Model 4: AdaBoost
set.seed(234)
gbm_mod = train(
  stroke ~ .,
  data = stroke_train,
  method = "gbm",
  distribution = "adaboost",
  tuneGrid = expand.grid(n.trees = c(500, 1000, 2000),
                         interaction.depth = 1:3,
                         shrinkage = c(0.001, 0.003, 0.005),
                         n.minobsinnode = 1:6),
  metric = "ROC",
  trControl = ctrl,
  verbose = FALSE
)

gbm_mod$bestTune
```

## Comparison

```{r}
comparison = 
  resamples(
    list(
      LASSO = lasso_mod,
      MARS = mars_mod,
      random_forest = rf_mod,
      adaboost = gbm_mod
    )
  )

summary(comparison)
bwplot(comparison, metric = "ROC")

# Model 4 (AdaBoost) was selected as the final model
```

## Test performance

```{r}
lasso_pred = predict(lasso_mod, newdata = stroke_test, type = "prob")[ , 2]
mars_pred = predict(mars_mod, newdata = stroke_test, type = "prob")[ , 2]
rf_pred = predict(rf_mod, newdata = stroke_test, type = "prob")[ , 2]
gbm_pred = predict(gbm_mod, newdata = stroke_test, type = "prob")[ , 2]

lasso_roc = roc(stroke_test$stroke, lasso_pred)
mars_roc = roc(stroke_test$stroke, mars_pred)
rf_roc = roc(stroke_test$stroke, rf_pred)
gbm_roc = roc(stroke_test$stroke, gbm_pred)

auc_mod = c(lasso_roc$auc[1], mars_roc$auc[1], rf_roc$auc[1], gbm_roc$auc[1])

plot(lasso_roc, col = 1, legacy.axes = TRUE)
plot(mars_roc, col = 2, add = TRUE)
plot(rf_roc, col = 3, add = TRUE)
plot(gbm_roc, col = 4, add = TRUE)

mod_names = c("Model 1 (LASSO)", "Model 2 (MARS)", "Model 3 (random forest)", "Model 4 (AdaBoost)")
legend("bottomright", legend = paste0(mod_names, ": ", round(auc_mod, 3)), col = 1:5, lwd = 2)
```

## Variable importance

```{r}
vip(lasso_mod$finalModel)
vip(mars_mod$finalModel)
vip(gbm_mod$finalModel)

set.seed(1)
rf_imp = ranger(
  stroke ~ ., 
  data = stroke_train, 
  mtry = rf_mod$bestTune[[1]], 
  splitrule = "gini",
  min.node.size = rf_mod$bestTune[[3]],
  importance = "impurity"
) 

barplot(sort(ranger::importance(rf_imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(8))
```